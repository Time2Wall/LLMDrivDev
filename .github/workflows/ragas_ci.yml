# GitHub Actions CI/CD Pipeline for Ragas LLM Testing
# File: .github/workflows/ragas_ci.yml

name: Ragas LLM Quality Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'hometask_9/**'
  pull_request:
    branches: [main]
    paths:
      - 'hometask_9/**'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.12'
  OLLAMA_MODEL: 'qwen2.5:1.5b'

jobs:
  setup-ollama:
    name: Setup Ollama
    runs-on: ubuntu-latest
    steps:
      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama server
        run: |
          ollama serve &
          sleep 5

      - name: Pull model
        run: |
          ollama pull ${{ env.OLLAMA_MODEL }}

  ragas-tests:
    name: Run Ragas Evaluation Tests
    runs-on: ubuntu-latest
    needs: setup-ollama
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama and pull model
        run: |
          ollama serve &
          sleep 5
          ollama pull ${{ env.OLLAMA_MODEL }}

      - name: Install dependencies
        run: |
          cd hometask_9
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Ragas evaluation notebook
        run: |
          cd hometask_9
          jupyter nbconvert --to notebook --execute hometask_9.ipynb \
            --ExecutePreprocessor.timeout=600 \
            --output hometask_9_executed.ipynb
        continue-on-error: false

      - name: Run pytest quality gates
        run: |
          cd hometask_9
          pytest tests/test_ragas_evaluation.py -v --tb=long --junitxml=test-results.xml

      # Quality Gate: Pipeline fails if metrics are below thresholds
      - name: Check quality gates
        run: |
          cd hometask_9
          python -c "
          import json, sys
          with open('ragas_results.json') as f:
              results = json.load(f)
          metrics = results['aggregate_metrics']
          thresholds = results['thresholds']
          failed = False
          for metric, threshold in thresholds.items():
              value = metrics.get(metric, 0)
              status = 'PASS' if value >= threshold else 'FAIL'
              print(f'{status}: {metric} = {value:.3f} (threshold: {threshold})')
              if value < threshold:
                  failed = True
          if failed:
              print('\nQuality gate FAILED!')
              sys.exit(1)
          print('\nAll quality gates PASSED!')
          "

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ragas-test-results
          path: |
            hometask_9/ragas_results.json
            hometask_9/evaluation_metrics.csv
            hometask_9/test-results.xml
            hometask_9/*.png

      - name: Upload executed notebook
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: executed-notebook
          path: hometask_9/hometask_9_executed.ipynb

  report:
    name: Generate Report
    runs-on: ubuntu-latest
    needs: ragas-tests
    if: always()

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: ragas-test-results
          path: results/

      - name: Generate summary
        run: |
          python -c "
          import json
          with open('results/ragas_results.json') as f:
              results = json.load(f)
          metrics = results['aggregate_metrics']
          print('## Ragas Evaluation Results')
          print('| Metric | Score | Threshold | Status |')
          print('|--------|-------|-----------|--------|')
          for metric, value in metrics.items():
              threshold = results['thresholds'].get(metric, 'N/A')
              status = 'Pass' if value >= threshold else 'Fail'
              print(f'| {metric} | {value:.3f} | {threshold} | {status} |')
          " >> $GITHUB_STEP_SUMMARY
