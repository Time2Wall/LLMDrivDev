{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application with Langfuse Observability\n",
    "\n",
    "## Homework 5: LLM Application with Comprehensive Tracing\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **RAG System** - Document loading and question answering\n",
    "2. **Local Ollama LLM** - Using qwen2.5:1.5b model\n",
    "3. **Langfuse Integration** - Full observability with traces, spans, generations, events, and scores\n",
    "4. **Datasets** - Creating test datasets for evaluation\n",
    "5. **Custom Evaluators** - LLM-as-a-judge and custom metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:11:34.115418Z",
     "start_time": "2026-01-24T11:11:34.111965Z"
    }
   },
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install langchain langchain-community langchain-ollama langchain-chroma chromadb langfuse python-dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:05.430773Z",
     "start_time": "2026-01-24T11:11:34.143427Z"
    }
   },
   "source": "import os\nimport time\nimport uuid\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\n\n# Langfuse - using decorator-based API (v2.x+)\nfrom langfuse import Langfuse\nfrom langfuse.decorators import observe, langfuse_context\n\n# LangChain\nfrom langchain_ollama import OllamaLLM, OllamaEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom langchain_core.prompts import PromptTemplate\n\nprint(\"All imports successful!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:05.438781Z",
     "start_time": "2026-01-24T11:12:05.434781Z"
    }
   },
   "source": [
    "# Configuration\n",
    "# For LOCAL Langfuse (Docker): http://localhost:3000\n",
    "# For CLOUD Langfuse: https://cloud.langfuse.com\n",
    "\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"pk-lf-your-public-key\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"sk-lf-your-secret-key\")\n",
    "\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"qwen2.5:1.5b\")\n",
    "\n",
    "# User ID for tracking\n",
    "USER_ID = f\"user_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "print(f\"Langfuse Host: {LANGFUSE_HOST}\")\n",
    "print(f\"Ollama Model: {OLLAMA_MODEL}\")\n",
    "print(f\"User ID: {USER_ID}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse Host: http://localhost:3000\n",
      "Ollama Model: qwen2.5:1.5b\n",
      "User ID: user_b706d106\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Langfuse Client\n",
    "\n",
    "Langfuse provides comprehensive observability for LLM applications:\n",
    "- **Traces**: Full execution path of requests\n",
    "- **Spans**: Individual operations within a trace\n",
    "- **Generations**: LLM calls with token usage\n",
    "- **Events**: Point-in-time occurrences\n",
    "- **Scores**: Quality and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:05.961447Z",
     "start_time": "2026-01-24T11:12:05.441862Z"
    }
   },
   "source": "# Initialize Langfuse client\n# Note: The decorator-based API uses environment variables or explicit configuration\nlangfuse = Langfuse(\n    host=LANGFUSE_HOST,\n    public_key=LANGFUSE_PUBLIC_KEY,\n    secret_key=LANGFUSE_SECRET_KEY,\n)\n\n# Configure the decorator context to use our Langfuse instance\nlangfuse_context.configure(\n    host=LANGFUSE_HOST,\n    public_key=LANGFUSE_PUBLIC_KEY,\n    secret_key=LANGFUSE_SECRET_KEY,\n)\n\nprint(\"Langfuse client initialized!\")\nprint(f\"Dashboard URL: {LANGFUSE_HOST}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Ollama LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:12.535353Z",
     "start_time": "2026-01-24T11:12:05.964452Z"
    }
   },
   "source": [
    "# Initialize Ollama LLM\n",
    "llm = OllamaLLM(\n",
    "    model=OLLAMA_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Initialize Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=OLLAMA_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    ")\n",
    "\n",
    "# Test LLM connection\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready!'\")\n",
    "print(f\"LLM Response: {test_response}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: Hello, I am ready!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Documents for RAG"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:12.544869Z",
     "start_time": "2026-01-24T11:12:12.539358Z"
    }
   },
   "source": [
    "# Sample documents about AI/ML topics\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Machine Learning Fundamentals\n",
    "        \n",
    "        Machine learning is a subset of artificial intelligence that enables systems to learn\n",
    "        and improve from experience without being explicitly programmed. There are three main\n",
    "        types of machine learning:\n",
    "        \n",
    "        1. Supervised Learning: The algorithm learns from labeled training data. Examples include\n",
    "           classification (predicting categories) and regression (predicting continuous values).\n",
    "           Common algorithms: Linear Regression, Decision Trees, Random Forest, SVM.\n",
    "        \n",
    "        2. Unsupervised Learning: The algorithm finds patterns in unlabeled data. Examples include\n",
    "           clustering (grouping similar items) and dimensionality reduction.\n",
    "           Common algorithms: K-Means, PCA, DBSCAN.\n",
    "        \n",
    "        3. Reinforcement Learning: The algorithm learns through trial and error, receiving rewards\n",
    "           or penalties for actions taken. Used in robotics, game playing, and autonomous vehicles.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"ml_fundamentals.txt\", \"topic\": \"machine_learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Deep Learning and Neural Networks\n",
    "        \n",
    "        Deep learning is a subset of machine learning based on artificial neural networks.\n",
    "        Key concepts include:\n",
    "        \n",
    "        - Neurons and Layers: Neural networks consist of interconnected nodes (neurons)\n",
    "          organized in layers: input, hidden, and output layers.\n",
    "        \n",
    "        - Activation Functions: Functions like ReLU, Sigmoid, and Tanh introduce non-linearity,\n",
    "          allowing networks to learn complex patterns.\n",
    "        \n",
    "        - Backpropagation: The algorithm used to train neural networks by calculating gradients\n",
    "          and adjusting weights to minimize the loss function.\n",
    "        \n",
    "        - Common Architectures:\n",
    "          * CNNs (Convolutional Neural Networks): Best for image processing\n",
    "          * RNNs (Recurrent Neural Networks): Best for sequential data\n",
    "          * Transformers: State-of-the-art for NLP tasks, basis for GPT and BERT\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"deep_learning.txt\", \"topic\": \"deep_learning\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Large Language Models (LLMs)\n",
    "        \n",
    "        Large Language Models are AI systems trained on vast amounts of text data. Key aspects:\n",
    "        \n",
    "        - Architecture: Most modern LLMs use the Transformer architecture, which relies on\n",
    "          self-attention mechanisms to process text efficiently.\n",
    "        \n",
    "        - Training: LLMs are trained on massive datasets using unsupervised learning, often\n",
    "          followed by fine-tuning with human feedback (RLHF).\n",
    "        \n",
    "        - Capabilities: Text generation, summarization, translation, question answering,\n",
    "          code generation, and reasoning tasks.\n",
    "        \n",
    "        - Examples: GPT-4, Claude, LLaMA, Qwen, Mistral\n",
    "        \n",
    "        - RAG (Retrieval-Augmented Generation): A technique that combines LLMs with external\n",
    "          knowledge retrieval to provide more accurate and up-to-date responses.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"llm_overview.txt\", \"topic\": \"llm\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"\n",
    "        Natural Language Processing (NLP)\n",
    "        \n",
    "        NLP is a field of AI focused on enabling computers to understand, interpret,\n",
    "        and generate human language. Key tasks include:\n",
    "        \n",
    "        - Tokenization: Breaking text into words or subwords\n",
    "        - Part-of-Speech Tagging: Identifying grammatical roles\n",
    "        - Named Entity Recognition: Identifying names, places, organizations\n",
    "        - Sentiment Analysis: Determining emotional tone\n",
    "        - Machine Translation: Converting between languages\n",
    "        - Text Summarization: Creating concise summaries\n",
    "        - Question Answering: Providing answers based on context\n",
    "        \n",
    "        Modern NLP heavily relies on transformer-based models like BERT for understanding\n",
    "        and GPT for generation tasks.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\"source\": \"nlp_basics.txt\", \"topic\": \"nlp\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\n",
    "for doc in SAMPLE_DOCUMENTS:\n",
    "    print(f\"  - {doc['metadata']['source']}: {doc['metadata']['topic']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 sample documents\n",
      "  - ml_fundamentals.txt: machine_learning\n",
      "  - deep_learning.txt: deep_learning\n",
      "  - llm_overview.txt: llm\n",
      "  - nlp_basics.txt: nlp\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Loading with Langfuse Tracing\n",
    "\n",
    "This section demonstrates:\n",
    "- Creating traces for operations\n",
    "- Using spans for sub-operations\n",
    "- Recording events\n",
    "- Adding scores"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:12.554096Z",
     "start_time": "2026-01-24T11:12:12.548230Z"
    }
   },
   "source": "@observe(name=\"document_indexing\")\ndef load_documents_with_tracing(documents: List[Dict]) -> Chroma:\n    \"\"\"\n    Load documents into vector store with comprehensive Langfuse tracing.\n    \n    Demonstrates:\n    - @observe decorator for automatic tracing\n    - Nested spans via nested @observe functions\n    - Events and scores via langfuse_context\n    \"\"\"\n    # Update trace with metadata\n    langfuse_context.update_current_trace(\n        user_id=USER_ID,\n        metadata={\n            \"num_documents\": len(documents),\n            \"model\": OLLAMA_MODEL,\n            \"operation\": \"document_loading\"\n        },\n        tags=[\"indexing\", \"rag\", \"setup\"]\n    )\n    \n    total_start = time.time()\n    \n    # Span 1: Text Splitting (nested observation)\n    chunks = split_documents(documents)\n    \n    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n    \n    # Span 2: Embedding Generation (nested observation)\n    vectorstore = create_embeddings(chunks)\n    \n    total_time = time.time() - total_start\n    \n    # Score: Processing quality\n    langfuse_context.score_current_trace(\n        name=\"indexing_success\",\n        value=1.0,\n        comment=f\"Successfully indexed {len(chunks)} chunks in {total_time:.2f}s\"\n    )\n    \n    trace_id = langfuse_context.get_current_trace_id()\n    print(f\"Indexing completed in {total_time:.2f}s\")\n    print(f\"Trace ID: {trace_id}\")\n    \n    return vectorstore\n\n\n@observe(name=\"text_splitting\")\ndef split_documents(documents: List[Dict]) -> List[Document]:\n    \"\"\"Split documents into chunks.\"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=50,\n        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n    )\n    \n    # Create Document objects\n    docs = [\n        Document(page_content=d[\"content\"], metadata=d[\"metadata\"])\n        for d in documents\n    ]\n    \n    # Split into chunks\n    chunks = text_splitter.split_documents(docs)\n    \n    langfuse_context.update_current_observation(\n        output={\n            \"num_chunks\": len(chunks),\n            \"avg_chunk_size\": sum(len(c.page_content) for c in chunks) / len(chunks)\n        }\n    )\n    \n    return chunks\n\n\n@observe(name=\"embedding_generation\")\ndef create_embeddings(chunks: List[Document]) -> Chroma:\n    \"\"\"Create embeddings and vector store.\"\"\"\n    embed_start = time.time()\n    \n    # Create vector store\n    vectorstore = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings,\n        collection_name=\"rag_demo\"\n    )\n    \n    embed_time = time.time() - embed_start\n    \n    langfuse_context.update_current_observation(\n        output={\"status\": \"success\", \"vectorstore_type\": \"chroma\"},\n        metadata={\"duration_seconds\": embed_time}\n    )\n    \n    return vectorstore",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T11:12:12.826620Z",
     "start_time": "2026-01-24T11:12:12.557101Z"
    }
   },
   "source": "# Load documents\nvectorstore = load_documents_with_tracing(SAMPLE_DOCUMENTS)\n\n# Flush to ensure data is sent to Langfuse\nlangfuse.flush()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Query with Full Instrumentation\n",
    "\n",
    "This demonstrates comprehensive tracing for RAG queries:\n",
    "- Main trace for the full query\n",
    "- Span for document retrieval\n",
    "- Generation for LLM call with token tracking\n",
    "- Events for key points\n",
    "- Scores for quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RAG Prompt Template\nRAG_PROMPT = PromptTemplate(\n    template=\"\"\"Use the following context to answer the question.\nIf you cannot find the answer in the context, say \"I don't have enough information.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n\n\n@observe(name=\"rag_query\")\ndef rag_query(\n    question: str,\n    vectorstore: Chroma,\n    session_id: Optional[str] = None,\n    k: int = 3\n) -> Dict[str, Any]:\n    \"\"\"\n    Execute a RAG query with comprehensive Langfuse tracing.\n    \n    Tracks:\n    - Execution time for all operations\n    - Input/output data\n    - Token usage (estimated)\n    - Retrieved documents and relevance\n    \"\"\"\n    session_id = session_id or f\"session_{uuid.uuid4().hex[:8]}\"\n    \n    # Update trace with metadata\n    langfuse_context.update_current_trace(\n        user_id=USER_ID,\n        session_id=session_id,\n        input=question,\n        metadata={\n            \"model\": OLLAMA_MODEL,\n            \"k\": k,\n            \"question_length\": len(question)\n        },\n        tags=[\"rag\", \"query\", \"qa\"]\n    )\n    \n    total_start = time.time()\n    \n    # Span: Document Retrieval (nested observation)\n    docs, context, sources, retrieval_time = retrieve_documents(question, vectorstore, k)\n    \n    # Generation: LLM Call (nested observation)\n    answer, generation_time, input_tokens, output_tokens = generate_answer(question, context)\n    \n    total_time = time.time() - total_start\n    \n    # Update trace with final output\n    langfuse_context.update_current_trace(\n        output=answer,\n        metadata={\n            \"total_time_seconds\": total_time,\n            \"retrieval_time_seconds\": retrieval_time,\n            \"generation_time_seconds\": generation_time\n        }\n    )\n    \n    # Score: Response quality (basic heuristic)\n    quality_score = min(1.0, len(answer) / 100) if len(answer) > 20 else 0.3\n    langfuse_context.score_current_trace(\n        name=\"response_quality\",\n        value=quality_score,\n        comment=f\"Auto-scored based on response length ({len(answer)} chars)\"\n    )\n    \n    trace_id = langfuse_context.get_current_trace_id()\n    \n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"sources\": sources,\n        \"context\": context,\n        \"trace_id\": trace_id,\n        \"metrics\": {\n            \"total_time\": total_time,\n            \"retrieval_time\": retrieval_time,\n            \"generation_time\": generation_time,\n            \"docs_retrieved\": len(docs),\n            \"context_length\": len(context),\n            \"input_tokens\": input_tokens,\n            \"output_tokens\": output_tokens\n        }\n    }\n\n\n@observe(name=\"document_retrieval\")\ndef retrieve_documents(question: str, vectorstore: Chroma, k: int):\n    \"\"\"Retrieve relevant documents for the question.\"\"\"\n    retrieval_start = time.time()\n    \n    # Retrieve documents\n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n    docs = retriever.invoke(question)\n    \n    retrieval_time = time.time() - retrieval_start\n    \n    # Build context\n    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n    sources = [doc.metadata for doc in docs]\n    \n    langfuse_context.update_current_observation(\n        input={\"question\": question, \"k\": k},\n        output={\n            \"num_docs\": len(docs),\n            \"context_length\": len(context),\n            \"sources\": sources\n        },\n        metadata={\"duration_seconds\": retrieval_time}\n    )\n    \n    return docs, context, sources, retrieval_time\n\n\n@observe(name=\"llm_answer_generation\", as_type=\"generation\")\ndef generate_answer(question: str, context: str):\n    \"\"\"Generate answer using LLM.\"\"\"\n    generation_start = time.time()\n    \n    # Update generation metadata\n    langfuse_context.update_current_observation(\n        model=OLLAMA_MODEL,\n        input={\n            \"prompt_template\": \"rag_qa\",\n            \"question\": question,\n            \"context_length\": len(context)\n        },\n        model_parameters={\"temperature\": 0.7}\n    )\n    \n    # Format prompt and call LLM\n    prompt = RAG_PROMPT.format(context=context, question=question)\n    answer = llm.invoke(prompt)\n    \n    generation_time = time.time() - generation_start\n    \n    # Estimate tokens (rough approximation)\n    input_tokens = int(len(prompt.split()) * 1.3)\n    output_tokens = int(len(answer.split()) * 1.3)\n    \n    langfuse_context.update_current_observation(\n        output=answer,\n        usage={\n            \"input\": input_tokens,\n            \"output\": output_tokens,\n            \"total\": input_tokens + output_tokens\n        },\n        metadata={\"duration_seconds\": generation_time}\n    )\n    \n    return answer, generation_time, input_tokens, output_tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test queries\nTEST_QUESTIONS = [\n    \"What are the three types of machine learning?\",\n    \"What is backpropagation and how does it work?\",\n    \"What is RAG and why is it useful?\",\n    \"What are common NLP tasks?\",\n    \"Explain the transformer architecture.\"\n]\n\nsession_id = f\"demo_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\nresults = []\n\nprint(\"Running test queries...\\n\")\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"Query {i}: {question}\")\n    print(\"-\" * 50)\n    \n    result = rag_query(question, vectorstore, session_id=session_id)\n    results.append(result)\n    \n    print(f\"Answer: {result['answer'][:300]}...\" if len(result['answer']) > 300 else f\"Answer: {result['answer']}\")\n    print(f\"\\nMetrics:\")\n    print(f\"  Total time: {result['metrics']['total_time']:.2f}s\")\n    print(f\"  Retrieval: {result['metrics']['retrieval_time']:.2f}s\")\n    print(f\"  Generation: {result['metrics']['generation_time']:.2f}s\")\n    print(f\"  Docs: {result['metrics']['docs_retrieved']}, Tokens: {result['metrics']['input_tokens']}+{result['metrics']['output_tokens']}\")\n    print(f\"  Trace ID: {result['trace_id']}\")\n    print(\"\\n\")\n\n# Flush to ensure data is sent\nlangfuse.flush()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Langfuse Datasets\n",
    "\n",
    "Datasets in Langfuse allow you to:\n",
    "- Store test cases for evaluation\n",
    "- Track expected outputs\n",
    "- Run experiments across multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for RAG evaluation\n",
    "DATASET_NAME = \"rag_evaluation_dataset\"\n",
    "\n",
    "# Create or get dataset\n",
    "dataset = langfuse.create_dataset(\n",
    "    name=DATASET_NAME,\n",
    "    description=\"Test dataset for RAG system evaluation\",\n",
    "    metadata={\n",
    "        \"created_by\": USER_ID,\n",
    "        \"version\": \"1.0\",\n",
    "        \"domain\": \"AI/ML knowledge\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test items with expected outputs\n",
    "TEST_ITEMS = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What are the three types of machine learning?\"},\n",
    "        \"expected_output\": \"supervised learning, unsupervised learning, and reinforcement learning\",\n",
    "        \"metadata\": {\"topic\": \"ml_basics\", \"difficulty\": \"easy\"}\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is backpropagation?\"},\n",
    "        \"expected_output\": \"algorithm for training neural networks by calculating gradients\",\n",
    "        \"metadata\": {\"topic\": \"deep_learning\", \"difficulty\": \"medium\"}\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is RAG?\"},\n",
    "        \"expected_output\": \"Retrieval-Augmented Generation combines LLMs with external knowledge retrieval\",\n",
    "        \"metadata\": {\"topic\": \"llm\", \"difficulty\": \"medium\"}\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"What are common activation functions?\"},\n",
    "        \"expected_output\": \"ReLU, Sigmoid, and Tanh\",\n",
    "        \"metadata\": {\"topic\": \"deep_learning\", \"difficulty\": \"easy\"}\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"Name some large language models.\"},\n",
    "        \"expected_output\": \"GPT-4, Claude, LLaMA, Qwen, Mistral\",\n",
    "        \"metadata\": {\"topic\": \"llm\", \"difficulty\": \"easy\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add items to dataset\n",
    "for item in TEST_ITEMS:\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        input=item[\"input\"],\n",
    "        expected_output=item[\"expected_output\"],\n",
    "        metadata=item[\"metadata\"]\n",
    "    )\n",
    "\n",
    "langfuse.flush()\n",
    "print(f\"Added {len(TEST_ITEMS)} items to dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Evaluator: LLM-as-a-Judge\n",
    "\n",
    "Create custom evaluation logic using LLM to assess response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LLM-as-a-Judge Evaluator\nJUDGE_PROMPT = PromptTemplate(\n    template=\"\"\"You are an expert evaluator. Assess the quality of the AI response.\n\nQuestion: {question}\nExpected Answer (key points): {expected}\nActual Response: {response}\n\nEvaluate on these criteria (score 0-10 for each):\n1. Relevance: Does it answer the question?\n2. Accuracy: Are the facts correct based on expected answer?\n3. Completeness: Does it cover key points?\n4. Clarity: Is it well-written and clear?\n\nProvide your evaluation as JSON:\n{{\n    \"relevance\": <score>,\n    \"accuracy\": <score>,\n    \"completeness\": <score>,\n    \"clarity\": <score>,\n    \"overall\": <average>,\n    \"explanation\": \"<brief explanation>\"\n}}\n\nJSON evaluation:\"\"\",\n    input_variables=[\"question\", \"expected\", \"response\"]\n)\n\n\n@observe(name=\"llm_judge_evaluation\")\ndef llm_judge_evaluate(\n    question: str,\n    expected: str,\n    response: str,\n    trace_id: str,\n) -> Dict[str, Any]:\n    \"\"\"\n    Use LLM as a judge to evaluate response quality.\n    \"\"\"\n    # Update trace metadata\n    langfuse_context.update_current_trace(\n        user_id=USER_ID,\n        metadata={\n            \"evaluated_trace_id\": trace_id,\n            \"evaluation_type\": \"llm_as_judge\"\n        },\n        tags=[\"evaluation\", \"llm-judge\"]\n    )\n    \n    # Call the judge generation\n    scores = judge_generation(question, expected, response)\n    \n    # Add scores to original trace using the langfuse client directly\n    overall_score = float(scores.get(\"overall\", 5)) / 10.0\n    \n    langfuse.score(\n        trace_id=trace_id,\n        name=\"llm_judge_overall\",\n        value=overall_score,\n        comment=scores.get(\"explanation\", \"LLM judge evaluation\")\n    )\n    \n    # Add individual metric scores if available\n    for metric in [\"relevance\", \"accuracy\", \"completeness\", \"clarity\"]:\n        if metric in scores:\n            langfuse.score(\n                trace_id=trace_id,\n                name=f\"llm_judge_{metric}\",\n                value=float(scores[metric]) / 10.0\n            )\n    \n    langfuse.flush()\n    \n    return scores\n\n\n@observe(name=\"judge_evaluation\", as_type=\"generation\")\ndef judge_generation(question: str, expected: str, response: str) -> Dict[str, Any]:\n    \"\"\"Generate judgment from LLM.\"\"\"\n    langfuse_context.update_current_observation(\n        model=OLLAMA_MODEL,\n        input={\n            \"question\": question,\n            \"expected\": expected,\n            \"response\": response[:500]  # Truncate for context\n        }\n    )\n    \n    # Call LLM judge\n    prompt = JUDGE_PROMPT.format(\n        question=question,\n        expected=expected,\n        response=response\n    )\n    \n    judge_response = llm.invoke(prompt)\n    \n    langfuse_context.update_current_observation(output=judge_response)\n    \n    # Parse scores (with fallback)\n    try:\n        # Try to extract JSON from response\n        import re\n        json_match = re.search(r'\\{[^{}]+\\}', judge_response, re.DOTALL)\n        if json_match:\n            scores = json.loads(json_match.group())\n        else:\n            scores = {\"overall\": 5.0, \"explanation\": \"Could not parse evaluation\"}\n    except:\n        scores = {\"overall\": 5.0, \"explanation\": \"Evaluation parsing failed\"}\n    \n    return scores\n\nprint(\"LLM-as-Judge evaluator defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run evaluation on our test results\nprint(\"Running LLM-as-Judge evaluation on results...\\n\")\n\nfor i, (result, test_item) in enumerate(zip(results[:3], TEST_ITEMS[:3]), 1):  # Evaluate first 3\n    print(f\"Evaluating Query {i}: {result['question']}\")\n    \n    scores = llm_judge_evaluate(\n        question=result['question'],\n        expected=test_item['expected_output'],\n        response=result['answer'],\n        trace_id=result['trace_id'],\n    )\n    \n    print(f\"  Evaluation: {scores}\")\n    print()\n\nlangfuse.flush()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Evaluators: Programmatic Metrics\n",
    "\n",
    "Create custom evaluators for specific metrics like:\n",
    "- Answer length\n",
    "- Keyword coverage\n",
    "- Source relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def custom_evaluators(\n    result: Dict[str, Any],\n    expected_keywords: List[str],\n) -> Dict[str, float]:\n    \"\"\"\n    Custom programmatic evaluators for RAG responses.\n    \"\"\"\n    trace_id = result['trace_id']\n    answer = result['answer'].lower()\n    \n    scores = {}\n    \n    # 1. Answer Length Score (normalized)\n    # Ideal length: 100-500 chars\n    length = len(result['answer'])\n    if length < 50:\n        length_score = 0.3\n    elif length < 100:\n        length_score = 0.6\n    elif length <= 500:\n        length_score = 1.0\n    else:\n        length_score = max(0.5, 1.0 - (length - 500) / 1000)\n    \n    scores['length_score'] = length_score\n    langfuse.score(\n        trace_id=trace_id,\n        name=\"answer_length\",\n        value=length_score,\n        comment=f\"Response length: {length} chars\"\n    )\n    \n    # 2. Keyword Coverage Score\n    keywords_found = sum(1 for kw in expected_keywords if kw.lower() in answer)\n    keyword_score = keywords_found / len(expected_keywords) if expected_keywords else 0\n    \n    scores['keyword_coverage'] = keyword_score\n    langfuse.score(\n        trace_id=trace_id,\n        name=\"keyword_coverage\",\n        value=keyword_score,\n        comment=f\"Found {keywords_found}/{len(expected_keywords)} keywords\"\n    )\n    \n    # 3. Response Time Score\n    total_time = result['metrics']['total_time']\n    if total_time < 2:\n        time_score = 1.0\n    elif total_time < 5:\n        time_score = 0.8\n    elif total_time < 10:\n        time_score = 0.6\n    else:\n        time_score = max(0.2, 1.0 - total_time / 30)\n    \n    scores['response_time'] = time_score\n    langfuse.score(\n        trace_id=trace_id,\n        name=\"response_time\",\n        value=time_score,\n        comment=f\"Total time: {total_time:.2f}s\"\n    )\n    \n    # 4. Context Utilization Score\n    # Check if answer uses information from context\n    context_words = set(result['context'].lower().split())\n    answer_words = set(answer.split())\n    overlap = len(context_words.intersection(answer_words))\n    context_score = min(1.0, overlap / 20)  # Normalize\n    \n    scores['context_utilization'] = context_score\n    langfuse.score(\n        trace_id=trace_id,\n        name=\"context_utilization\",\n        value=context_score,\n        comment=f\"Word overlap with context: {overlap}\"\n    )\n    \n    langfuse.flush()\n    \n    return scores\n\nprint(\"Custom evaluators defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply custom evaluators to results\nEXPECTED_KEYWORDS = {\n    0: [\"supervised\", \"unsupervised\", \"reinforcement\"],\n    1: [\"backpropagation\", \"gradient\", \"neural\", \"weights\"],\n    2: [\"retrieval\", \"augmented\", \"generation\", \"llm\"],\n    3: [\"tokenization\", \"sentiment\", \"translation\", \"nlp\"],\n    4: [\"transformer\", \"attention\", \"gpt\", \"bert\"]\n}\n\nprint(\"Running custom evaluations...\\n\")\n\nfor i, result in enumerate(results):\n    keywords = EXPECTED_KEYWORDS.get(i, [])\n    scores = custom_evaluators(result, keywords)\n    \n    print(f\"Query {i+1}: {result['question'][:50]}...\")\n    print(f\"  Length: {scores['length_score']:.2f}\")\n    print(f\"  Keywords: {scores['keyword_coverage']:.2f}\")\n    print(f\"  Time: {scores['response_time']:.2f}\")\n    print(f\"  Context: {scores['context_utilization']:.2f}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Running Dataset Experiment\n",
    "\n",
    "Run the RAG system against the dataset and track results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_dataset_experiment(\n    dataset_name: str,\n    run_name: str,\n    vectorstore: Chroma,\n):\n    \"\"\"\n    Run experiment against a dataset.\n    \"\"\"\n    # Get dataset items\n    dataset = langfuse.get_dataset(dataset_name)\n    \n    print(f\"Running experiment '{run_name}' on dataset '{dataset_name}'\")\n    print(f\"Total items: {len(dataset.items)}\\n\")\n    \n    experiment_results = []\n    \n    for item in dataset.items:\n        question = item.input.get(\"question\", \"\")\n        expected = item.expected_output\n        \n        print(f\"Processing: {question[:50]}...\")\n        \n        # Run RAG query\n        result = rag_query(\n            question=question,\n            vectorstore=vectorstore,\n            session_id=f\"experiment_{run_name}\"\n        )\n        \n        # Link to dataset run\n        item.link(\n            trace_id=result['trace_id'],\n            run_name=run_name\n        )\n        \n        # Simple accuracy check\n        expected_lower = expected.lower() if expected else \"\"\n        answer_lower = result['answer'].lower()\n        \n        # Check keyword overlap\n        expected_words = set(expected_lower.split())\n        answer_words = set(answer_lower.split())\n        overlap = len(expected_words.intersection(answer_words))\n        accuracy = overlap / len(expected_words) if expected_words else 0\n        \n        # Score the result\n        langfuse.score(\n            trace_id=result['trace_id'],\n            name=\"dataset_accuracy\",\n            value=min(1.0, accuracy),\n            comment=f\"Keyword overlap: {overlap}/{len(expected_words)}\"\n        )\n        \n        experiment_results.append({\n            \"question\": question,\n            \"expected\": expected,\n            \"actual\": result['answer'][:200],\n            \"accuracy\": accuracy,\n            \"trace_id\": result['trace_id']\n        })\n        \n        print(f\"  Accuracy: {accuracy:.2f}\")\n    \n    langfuse.flush()\n    \n    # Summary\n    avg_accuracy = sum(r['accuracy'] for r in experiment_results) / len(experiment_results)\n    print(f\"\\n{'='*50}\")\n    print(f\"Experiment Complete: {run_name}\")\n    print(f\"Average Accuracy: {avg_accuracy:.2%}\")\n    print(f\"{'='*50}\")\n    \n    return experiment_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run the experiment\nexperiment_results = run_dataset_experiment(\n    dataset_name=DATASET_NAME,\n    run_name=f\"rag_v1_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n    vectorstore=vectorstore,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Human Annotation Support\n",
    "\n",
    "Add scores that can be used for human annotation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def add_human_annotation_placeholder(trace_id: str):\n    \"\"\"\n    Add placeholder scores that can be updated through human annotation in Langfuse UI.\n    \"\"\"\n    # These scores can be updated via Langfuse UI annotation queues\n    annotation_categories = [\n        (\"human_accuracy\", \"Is the answer factually correct?\"),\n        (\"human_helpfulness\", \"Is the answer helpful and complete?\"),\n        (\"human_relevance\", \"Does it directly answer the question?\"),\n        (\"human_safety\", \"Is the response appropriate and safe?\")\n    ]\n    \n    for score_name, comment in annotation_categories:\n        langfuse.score(\n            trace_id=trace_id,\n            name=score_name,\n            value=0.5,  # Placeholder - to be updated by human annotator\n            comment=f\"PENDING HUMAN REVIEW: {comment}\"\n        )\n    \n    langfuse.flush()\n    print(f\"Added annotation placeholders for trace {trace_id}\")\n\n# Add annotation placeholders to first result\nif results:\n    add_human_annotation_placeholder(results[0]['trace_id'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Dashboard Information\n",
    "\n",
    "Access Langfuse dashboard to view:\n",
    "- **Traces**: Full query execution paths\n",
    "- **Observations**: Individual operations (spans, generations)\n",
    "- **Dashboards**: Aggregate metrics and trends\n",
    "- **Datasets**: Test cases and experiments\n",
    "- **Annotation Queues**: Human review workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final flush\n",
    "langfuse.flush()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAG APPLICATION WITH LANGFUSE - COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLangfuse Dashboard: {LANGFUSE_HOST}\")\n",
    "print(f\"\\nKey entities to explore:\")\n",
    "print(f\"  - Traces: View full execution paths\")\n",
    "print(f\"  - Observations: See spans and generations\")\n",
    "print(f\"  - Scores: Review quality metrics\")\n",
    "print(f\"  - Datasets: {DATASET_NAME}\")\n",
    "print(f\"  - Sessions: demo_session_*\")\n",
    "print(f\"\\nUser ID for filtering: {USER_ID}\")\n",
    "print(f\"\\nTest queries executed: {len(results)}\")\n",
    "print(f\"Dataset items: {len(TEST_ITEMS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trace IDs for easy access\n",
    "print(\"\\nTrace IDs for review:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. {result['trace_id']} - {result['question'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Langfuse UI Screenshots Guide\n",
    "\n",
    "After running this notebook, take screenshots of:\n",
    "\n",
    "1. **Traces (General View)**: Shows all traces with timing and status\n",
    "2. **Trace Detail (Expanded)**: Shows spans, generations, and events within a trace\n",
    "3. **Observations**: Individual spans and generations\n",
    "4. **Dashboards**: Aggregate metrics, latency, token usage\n",
    "5. **LLM-as-Judge**: Traces with evaluation scores\n",
    "6. **Annotation Queue**: Human review workflow (if configured)\n",
    "7. **Custom Evaluator**: Score distributions and trends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}